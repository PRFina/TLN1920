{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "739282abffc8079465c7416af47f8b6d2681da148d602a4884c4032805e054cf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "import src.data_manager as dm\n",
    "import src.text_summarization as summ\n",
    "import src.topic_extraction as te"
   ]
  },
  {
   "source": [
    "# Text summarization\n",
    "\n",
    "In questa esercitazione vederemo come effettuare un task di single document text summarization avvalendoci dell'utilizzo di *Nasari*. Un task di text summarization prevede di definire un mapping $s: D \\to D$ che mappa un documento in input $d \\in D$ ad una sua versione sommaria $d^* \\in D$. Le informazioni contenute in $d^*$ e mantenute rispetto a $d$ variano a seconda del contesto, obiettivi, requisiti e condizioni operative. \n",
    "\n",
    "In generale $|d^*| < |d|$\n",
    "\n",
    "In questa esercitazione, il mapping $s(d) = d^*$, è definito in base alla seguente procedura di tipo **estrattivo**:\n",
    "\n",
    "1. Estrazione del topic del titolo.\n",
    "2. Estrazione del topic per ogni chunk segmentato dal corpo del testo.\n",
    "3. Calcolo della rilevanza del topic espresso dal chunk rispetto a quello del documento.\n",
    "4. Selezione e riordino dei chunks più rilevanti.\n",
    "\n",
    "L'idea alla base della procedura consiste nel estrarre il **topic**l, ovvero una rappresentazione sommaria e coincisa delle informazioni espresse in una certa unità del testo. Nel nostro caso il topic è rappresentato da un'**insieme di vettori Nasari**. Una volta estratto il topic secondo una qualche strategia (in questo caso dal titolo), confrontiamo quanto le differenti \"parti del testo\" (chunks) siano **rilevanti rispetto** al topic uutilizzando come  misura di similarità la *Weighted Overlap*.\n",
    "\n",
    "I chunks che andranno a costituire il documento in output $d^*$ saranno solo quelli più rilevanti rispetto al topic e che (si spera) preservino le informazioni presenti nel documento originale $d$. In questo caso parliamo di \"chunks\" poiché la segmentazione e analisi della rilevanza del testo può avvenire a differenti livelli: parole, frasi, paragrafi, ecc...\n",
    "\n",
    "### Nasari\n",
    "\n",
    "Nasari è una risorsa lessicale simile a *Babelnet*, che a differenza di quest'ultima, utilizza una rappresentazione vettoriale. I vettori presenti in Nasari reppresentano lo **span** di uno spazio multi-dimensionale in cui ogni word-form rappresenta una dimensione di tale spazio. Lo spazio è **sparso** in quanto ogni vettore un numero di componenti (lemmi) differenti.\n",
    "\n",
    "Vediamo alcuni esempi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lemma: bottle; vector: [('bottle', 2225.82), ('glass', 634.59), ('wine', 380.36), ('chianti', 302.56), ('plastic', 285.24), ('water bottle', 152.6), ('pet', 139.22), ('container', 138.82), ('cork', 135.04), ('beer', 131.9), ('soft drink', 115.05), ('use', 105.64), ('privy', 92.28), ('jug', 92.08)]\n\nlemma: money; vector: [('money', 1475.44), ('coin', 1242.3), ('currency', 1239.76), ('gold', 1074.39), ('value', 704.1), ('commodity', 596.05), ('monetary', 436.14), ('dollar', 427.6), ('silver', 385.59), ('gold standard', 363.72), ('money supply', 327.35), ('exchange', 305.08), ('central bank', 300.59), ('marx', 300.29)]\n\nlemma: bank; vector: [('bank', 2131.9), ('banking', 479.31), ('deposit', 229.42), ('credit union', 160.08), ('money', 142.3), ('loan', 138.98), ('commercial bank', 137.9), ('central bank', 121.84), ('customer', 87.97), ('federal reserve', 77.17), ('depositor', 77.03), ('branch', 75.26), ('cheque', 74.7), ('fractional-reserve', 71.23)]\n\n"
     ]
    }
   ],
   "source": [
    "nasari = dm.Nasari(Path('data/dd-small-nasari-15.txt'))\n",
    "lemmas = ['bottle', 'money', 'bank']\n",
    "\n",
    "for lemma in lemmas:\n",
    "    print(\"lemma: {}; vector: {}\\n\".format(lemma, nasari.get_vector(lemma)))"
   ]
  },
  {
   "source": [
    "Come si può osservare ad ogni lemma è associato un vettore nasari, ovvero una lista di coppie (lemma, score), lo score viene calcolato con la *lexical specificity* dalla processo di costruzione di Nasari.\n",
    "\n",
    "\n",
    "### Topic Extraction\n",
    "Vediamo ora cosa significa, nella pratica, estrarre il topic del documento utilizzando una strategia *title-based*. \n",
    "\n",
    "Esaminiamo il documento *\"Ebola-Virus-disease\"*:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document Title: Ebola virus disease\n",
      "\n",
      "Document Topic: [[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)], [('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]]\n"
     ]
    }
   ],
   "source": [
    "doc = dm.parse_document_sentence(Path('data/text-documents/Ebola-virus-disease.txt'))\n",
    "title_extractor = te.TitleExtractor(nasari)\n",
    "print(\"Document Title: {}\\n\".format(doc.title))\n",
    "main_topic = title_extractor.get_topic(doc)\n",
    "print(\"Document Topic: {}\".format(main_topic))"
   ]
  },
  {
   "source": [
    "Come si può osservare dall'output, il topic del documento è rappresentaro dai 2 vettori nasari associati ai lemmi \"virus\" e \"disease\". La parola \"Ebola\" presente nel titolo non ha un corrispettivo vettore nasari associato. \n",
    "\n",
    "Estraendo il topic anche dei chunks presenti nel corpo del testo possiamo quantificare quanto ogni chunk sia rilevante per il topic del docuemento calcolando la weighted overlap tra i vettori nasari. \n",
    "\n",
    "Vediamo un esempio:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document topic: 2, Sentence topic: 5\n"
     ]
    }
   ],
   "source": [
    "sentence_topic = te.TopicExtractor(nasari).get_topic(doc.body[0]) # just the first chunk (here sentence)\n",
    "print(\"Document topic: {}, Sentence topic: {}\".format(len(main_topic), len(sentence_topic)))"
   ]
  },
  {
   "source": [
    "Dato che i due topic sono costituiti da molteplici vettori nasari, per quantificare la rilevanza del chunk possiamo calcolare la WO media tra tutte le possibili coppie di vettori.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)]\n[('lemur', 2060.13), ('primate', 2038.41), ('ape', 456.84), ('monkey', 407.01), ('human', 376.41), ('tarsier', 369.35), ('chimpanzee', 299.83), ('strepsirrhines', 292.3), ('specie', 287.8), ('ring-tailed lemur', 272.83), ('prosimian', 247.4), ('madagascar', 186.63), ('mya', 173.93), ('strepsirrhini', 161.09)]\n\n[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)]\n[('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]\n\n[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)]\n[('fever', 920.08), ('temperature', 473.72), ('body temperature', 317.02), ('hyperthermia', 289.22), ('malaria', 280.04), ('symptom', 241.55), ('aspirin', 238.77), ('hypothermia', 205.55), ('infection', 200.23), ('disease', 192.61), ('heat', 176.15), ('patient', 135.12), ('thermometer', 129.53), ('inflammation', 129.47)]\n\n[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)]\n[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)]\n\n[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)]\n[('human', 2413.82), ('chimpanzee', 957.62), ('neanderthal', 741.17), ('homo', 384.18), ('primate', 381.19), ('homo sapiens', 353.0), ('genetic', 323.6), ('homo erectus', 310.61), ('h.', 307.25), ('ape', 284.57), ('fossil', 282.81), ('modern', 268.9), ('evolutionary', 249.84), ('evolution', 242.99)]\n\n[('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]\n[('lemur', 2060.13), ('primate', 2038.41), ('ape', 456.84), ('monkey', 407.01), ('human', 376.41), ('tarsier', 369.35), ('chimpanzee', 299.83), ('strepsirrhines', 292.3), ('specie', 287.8), ('ring-tailed lemur', 272.83), ('prosimian', 247.4), ('madagascar', 186.63), ('mya', 173.93), ('strepsirrhini', 161.09)]\n\n[('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]\n[('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]\n\n[('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]\n[('fever', 920.08), ('temperature', 473.72), ('body temperature', 317.02), ('hyperthermia', 289.22), ('malaria', 280.04), ('symptom', 241.55), ('aspirin', 238.77), ('hypothermia', 205.55), ('infection', 200.23), ('disease', 192.61), ('heat', 176.15), ('patient', 135.12), ('thermometer', 129.53), ('inflammation', 129.47)]\n\n[('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]\n[('disease', 2394.01), ('health', 334.63), ('infection', 281.32), ('symptom', 200.22), ('patient', 197.26), ('cause', 190.62), ('treatment', 180.2), ('chronic', 178.71), ('pathogen', 166.77), ('kawasaki disease', 154.51), ('epidemiology', 154.45), ('ménière', 151.47), ('condition', 138.45), ('disorder', 135.0)]\n\n[('virus', 5745.67), ('viral', 1300.78), ('cell', 1172.39), ('genome', 1172.14), ('rna', 810.54), ('protein', 780.34), ('dna', 638.26), ('infect', 518.51), ('host', 462.58), ('infection', 454.04), ('capsid', 432.66), ('replication', 404.06), ('gene', 382.61), ('disease', 299.15)]\n[('human', 2413.82), ('chimpanzee', 957.62), ('neanderthal', 741.17), ('homo', 384.18), ('primate', 381.19), ('homo sapiens', 353.0), ('genetic', 323.6), ('homo erectus', 310.61), ('h.', 307.25), ('ape', 284.57), ('fossil', 282.81), ('modern', 268.9), ('evolutionary', 249.84), ('evolution', 242.99)]\n\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for v1, v2 in itertools.product(main_topic, sentence_topic):\n",
    "    print(v1)\n",
    "    print(v2)\n",
    "    print()"
   ]
  },
  {
   "source": [
    "Come si può osservare dall'output precedente, abbiamo 10 possibili coppie, quindi 10 differenti score di WO. Calcolando la media otteniamo un unico score che indica la rilevanze del chunk rispetto al topic del documento.\n",
    "\n",
    "Ripetendo questa operazione per tutti i chunks estratti dal corpo del documento possiamo creare un ranking in base alla rilevanza e quindi selezionare solo i top-k chunks più rilevanti. La logica appena espressa è codificata nei vari metodi della classe `TextSummarizer`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The lockdown has also prompted many people to reassess their lives and what is most important to them, bringing unexpected realisations and touching moments with their families. [0.291;5]\nAs the world endures lockdown to slow the spread of Covid-19, people from Hong Kong to Italy are trying to find a way to continue their normal lives [0.250;1]\nAsked what is the first thing she wants when life get back to normal, she says: “A hug.” [0.250;31]\nAfter life returns to normal, I think the first thing for me is to have a big meal in a decent restaurant. [0.167;20]\nIn the Venezuelan capital Caracas, 51-year-old Ana Pereira lives alone with her dog and cat. [0.167;27]\nThe closure of workplaces has given people time with their families they never had before. [0.166;16]\nMillions of people worldwide are having to embrace life under lockdown – confined to their own four walls or neighbourhoods for weeks on end as countries battle to reduce the spread of the coronavirus. [0.138;2]\nIn the United States, as in other countries struck by the virus, Dr William Jason Sulaka has learned how to conduct consultations online as he can no longer meet his patients face to face. [0.107;13]\nShe is sitting down in front of her computer to a virtual picnic with friends, as they can’t actually meet as they have done weekly since 2011. [0.089;28]\nDino Lin, a 40-year-old who works in an auto-part manufacturer, was lucky enough to move into a more spacious apartment in Shanghai just before the virus took hold, allowing his 5-year-old daughter Wowo Lin to have her own room. [0.071;17]\nThomas Law Kwok Fai, a 70-year-old Catholic priest in Hong Kong, has also turned to livestreaming, after the diocese temporarily suspended public masses at churches. [0.070;23]\nThis new way of living poses huge challenges. [0.000;3]\n"
     ]
    }
   ],
   "source": [
    "doc = dm.parse_document_sentence(Path('data/text-documents/Life-indoors.txt'))\n",
    "\n",
    "summarizer = summ.TextSummarizer(main_extractor=te.TitleExtractor(nasari),\n",
    "                                 chunk_extractor=te.TopicExtractor(nasari))\n",
    "\n",
    "summary = summarizer.get_summary(doc, compression_ratio=60, debug=True)\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "source": [
    "Nella cella precedente i chunk vengono estratti a livello di frase. Sebbene si abbia una  **maggiore granularità** nel calcolo delle informazioni rilevanti, si può osservare come viene a mancare una certa **coerenza** nel testo prodotto. Il sommario risulta quindi una **semplice giustapposizione** di frasi.\n",
    "\n",
    "Vediamo invece cosa accade quando cambiamo il livello di analisi passando dalla frase al paragrafo:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This new way of living poses huge challenges. Teaching, working and socialising have moved online as never before. The lockdown has also prompted many people to reassess their lives and what is most important to them, bringing unexpected realisations and touching moments with their families. [0.291;3]\nAs the world endures lockdown to slow the spread of Covid-19, people from Hong Kong to Italy are trying to find a way to continue their normal lives [0.250;1]\nAsked what is the first thing she wants when life get back to normal, she says: “A hug.” [0.250;12]\nMillions of people worldwide are having to embrace life under lockdown – confined to their own four walls or neighbourhoods for weeks on end as countries battle to reduce the spread of the coronavirus. [0.138;2]\nIn the Venezuelan capital Caracas, 51-year-old Ana Pereira lives alone with her dog and cat. She is sitting down in front of her computer to a virtual picnic with friends, as they can’t actually meet as they have done weekly since 2011. It is a poor replacement. “I need physical contact and I’m missing it a lot,” she says. [0.128;11]\n"
     ]
    }
   ],
   "source": [
    "doc = dm.parse_document_paragraph(Path('data/text-documents/Life-indoors.txt'))\n",
    "\n",
    "summarizer = summ.TextSummarizer(main_extractor=te.TitleExtractor(nasari),\n",
    "                                 chunk_extractor=te.TopicExtractor(nasari))\n",
    "\n",
    "summary = summarizer.get_summary(doc, compression_ratio=60, debug=True)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "source": [
    "Da una valutazione soggettiva, il risultato prodotto lavorando a livello di paragrafi sembra più coerente e meno fragmentato rispetto all'esperimento precedente. L'osservazione non sorprende in quanto maggiore è il livello di granularità di analisi, maggiore sarà la sofisticatezza richiesta nella fase di riordinamento e riassemblaggio dei chunks estratti.\n",
    "\n",
    "Per il semplicissimo approccio di riassemblaggio dei chunks qui implementato, non stupisce come una segmentazione a livello di paragrafo risulti qualitativamnete migliore."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Batch Processing\n",
    "\n",
    "Effettuiamo ora un esperimento su 4 differenti testi e con 4 livelli di compressione differenti. I testi generati sono visibili nella cartella `output`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_paths = Path('data/text-documents/').glob('*.txt')\n",
    "output_path = Path('output')\n",
    "output_path.mkdir(exist_ok=True) \n",
    "compression_levels = [10, 30, 60, 90]\n",
    "\n",
    "for doc_path in doc_paths:\n",
    "    doc = dm.parse_document_paragraph(doc_path)\n",
    "    out_file =  output_path / doc_path.stem \n",
    "    with out_file.open('w') as file:\n",
    "        for compression in compression_levels:\n",
    "            summary = summarizer.get_summary(doc, compression_ratio=compression, debug=True)\n",
    "            file.write(\"Compression Level: {}%\\n\".format(compression))\n",
    "            file.write(summary)\n",
    "            file.write(\"\\n_____________________________________________________________\\n\".format(compression))\n",
    "            "
   ]
  },
  {
   "source": [
    "## Risultati\n",
    "\n",
    "In generale analizzando i valori di relevance ottenuti per ciascun chunks emerge un'importante dinamica. I documenti come \"Andy-Warhol\" e \"Napoleon-wiki\" presentano valori di rilevance molto bassi in quanto il titolo essenzialmente è composto da una singola *named entity* dal quale non si riescono ad estrarre vettori nasari significativi che rappresentino il topic, e che di conseguenza, portano ad un basso overlap.\n",
    "\n",
    "Analizzando qualitativamente il contenuto dei due testi, emerge un ulteriore aspetto. I due testi in questione presentano ricorrenti menzioni ad entità (luoghi, date, eventi)ed hanno un contenuto in stile **narrattivo** piuttosto che **fattuale** come i documenti \"Ebola-virus-disease\" e \"Life-indoors\". Questo aspetto potrebbe mettere in difficoltà la procedura di summarization implementata.\n",
    "\n",
    "In generale anche in questo task, si può apprezzare come il **coverage** della risorsa lessicale utilizzata abbia un forte impatto sul risultato finale prodotto.\n",
    "\n",
    "### Napoleon-wiki document\n",
    "\n",
    "Questo documento presenta una peculiarità rispetto agli altri sommari generati. Come si può osservare dall'output generato i valori di rilevanza dei paragrafi sono sempre $0$, dinamica che si riflette anche sull'ordine dei paragrafi nel sommario, uguale all'ordine originale. Questo fenomeno è imputabile principalmente alla tipologia del titolo del documento in quanto contiene solamente una word-form (\"Napoleon\") di una *named entity* non presente in Nasari e dunque producendo un overlap vuoto.   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}