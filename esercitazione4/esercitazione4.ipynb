{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('TLN': conda)",
   "metadata": {
    "interpreter": {
     "hash": "739282abffc8079465c7416af47f8b6d2681da148d602a4884c4032805e054cf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import src.data_manager as dm\n",
    "import src.sense_similarity as sim"
   ]
  },
  {
   "source": [
    "# Esercitazione 4\n",
    "In questa esercitazione vedremo come utilizzare Nasari per un task di **sense similarity**. Nello specifico utilizzeremo la versione Nasari **embedded**, una versione in cui i concetti sono rappresentati in uno spazio embedded di 300 dimensioni.\n",
    "\n",
    "Oltre al task principale ci occuperemo anche di annotare coppie di parole. L'esercitazione si divide in due task principali:\n",
    "* Task 1: annotazione e valutazione dello score di similarit√†.\n",
    "* Task 2: annotazione e valutazione dei sensi.\n",
    "\n",
    "Come primo step analizziamo le 2 risorse lessicali principalmente utilizzate:\n",
    "\n",
    "* Mapping lemma-to-synsets basato sul corpus *SemEval2017ITA*\n",
    "* Nasari (versione embedded)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "semeval = dm.SemEval(Path('data/SemEval17_IT_senses2synsets.txt'))\n",
    "\n",
    "for lemma in ['agrume','bicicletta','prete']:\n",
    "    senses_id = semeval.get_synsetsID('agrume')\n",
    "    print(f\"Il lemma: '{lemma}' ha i seguenti possibili babel synsets: {senses_id}\")"
   ]
  },
  {
   "source": [
    "la classe `SemEval` rappresenta un **mapping** tra un lemma e alcuni dei synset disponibili in *BabelNet* associati ad esso.\n",
    "La classe `Nasari` permette di accedere attraverso una semplice API alla risorsa lessicale omonima. \n",
    "\n",
    "Ad esempio dato un babel synset id possiamo recuperare la rappresentazione vettoriale associata:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasari = dm.Nasari(Path('data/mini_NASARI.tsv'), mapper=semeval)\n",
    "nasari.get_vector('bn:00019301n') # first sense of 'agrume' lemma"
   ]
  },
  {
   "source": [
    "Possiamo inoltre, sfruttando il mapper `semeval`, recuperare tutti i babel synsets e di conseguenza i vettori associati ad un lemma: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Il lemma: 'agrume' ha i seguenti possibili babel synsets: ['bn:00019301n', 'bn:00019305n', 'bn:15303858n']\n\nRappresentazione embedded:\nbn:00019301n:[ 6.9381000e-04  7.4659700e-03 -1.8382970e-02  1.1926720e-01\n -4.7373670e-02  6.9858100e-03  4.3458340e-02 -1.0989362e-01\n  2.8266000e-04  1.5510333e-01  9.4744760e-02 -1.5407147e-01\n -8.8896630e-02  6.6979250e-02 -2.0702154e-01  1.3025508e-01\n -1.7170539e-01  2.1270724e-01 -3.2214940e-02  2.0095530e-02\n -8.8052400e-03  4.4744130e-02  5.5434610e-02  3.3728700e-03\n -3.2934440e-02 -7.8821210e-02 -1.1223518e-01  4.3777560e-02\n -2.4267820e-02  6.6450460e-02 -4.6993650e-02 -2.2482930e-02\n  6.3626380e-02  6.3381300e-03 -3.3713290e-02  4.7530030e-02\n  2.6722000e-03 -1.8323416e-01  1.0985690e-01  9.4298880e-02\n  1.3884781e-01 -1.4735948e-01  1.4661580e-02  4.3326490e-02\n -8.6036600e-02 -2.8626553e-01 -2.5982410e-02 -3.7649540e-02\n -7.7904390e-02  6.0011560e-02 -1.2072667e-01  5.8185260e-02\n  7.1826710e-02 -5.4374200e-03 -2.5897080e-02  7.5925530e-02\n -4.2881150e-02  3.5434590e-02 -6.1172790e-02 -1.1974332e-01\n -1.1427394e-01 -1.1480300e-02 -6.9593420e-02 -2.1493340e-02\n -3.7841160e-02 -1.4481717e-01 -3.9428900e-03 -1.6121118e-01\n  1.2418385e-01 -1.1487400e-02  8.3713640e-02 -1.2516980e-02\n -5.7049090e-02 -8.8304330e-02 -2.2713012e-01  1.8609420e-02\n -2.3428680e-02 -1.0921006e-01 -2.3115950e-02  4.2794140e-02\n -9.7396480e-02  1.2936560e-02 -8.8533600e-03 -1.4399293e-01\n  3.5072900e-02  8.9801000e-02 -1.3185475e-01  2.2759434e-01\n -7.0566140e-02  1.0056846e-01 -2.2145380e-02 -8.7806670e-02\n  1.9371430e-02  1.0472800e-03 -3.1220830e-02 -1.8102350e-02\n  2.8952930e-02  2.6926350e-02  7.5718070e-02 -1.1653640e-02\n -1.8283842e-01 -6.7297180e-02  3.6853550e-02  2.9261980e-02\n -7.4895770e-02 -1.0903063e-01  8.7961190e-02  1.2540544e-01\n -1.5199326e-01 -1.6256284e-01 -2.2763038e-01 -1.5708389e-01\n  6.9015140e-02  7.1428370e-02  3.9132710e-02  6.1626840e-02\n  3.4575360e-02 -5.9561000e-02  2.4659590e-02 -1.3402720e-02\n -3.5267310e-02  6.4538330e-02 -4.0047970e-02 -5.4215660e-02\n  7.1652400e-02  5.1847200e-02 -7.8505190e-02  3.1888750e-02\n  1.3586508e-01  1.6767356e-01 -1.0523947e-01 -2.0824380e-02\n  4.6803650e-02 -2.9524950e-02  5.9256900e-03  1.9909827e-01\n -4.6695460e-02  8.1009200e-03  1.1698485e-01 -8.0497900e-03\n -2.0752610e-02 -2.2143054e-01 -1.8725054e-01 -6.0931490e-02\n  8.8461200e-02 -1.6749350e-02  1.0504142e-01  1.4513286e-01\n -2.9349443e-01 -4.3551660e-02  1.7441282e-01 -6.8164440e-02\n -1.5215038e-01  1.1740788e-01 -2.4768170e-02 -1.1037977e-01\n -4.3605990e-02 -1.4368068e-01 -6.5309690e-02 -1.0492220e-02\n -9.3621630e-02 -1.4065060e-02  1.1097175e-01  5.6595030e-02\n  5.7582650e-02 -9.6839330e-02  1.3368145e-01 -2.4277870e-02\n -6.9757320e-02 -2.4613320e-02 -1.0507116e-01  1.4044766e-01\n -7.0035600e-02 -6.9612090e-02 -1.7889455e-01 -7.5052200e-02\n  1.3773641e-01 -1.3090538e-01 -3.4969260e-02  8.0976040e-02\n -1.6805470e-01 -7.8762560e-02 -1.1149560e-01  8.7808320e-02\n -1.2908458e-01 -8.3639980e-02 -2.8811660e-02  1.2795850e-02\n -5.2226700e-02  3.8381580e-02 -1.1923884e-01  8.3204270e-02\n -1.1342745e-01 -4.3974930e-02  1.3236028e-01  2.1058339e-01\n  2.9994730e-02 -4.0730620e-02  2.8292600e-03 -1.5234723e-01\n  9.7597880e-02 -1.2689729e-01 -1.9402010e-02  7.5997150e-02\n -7.2585760e-02  1.1469720e-02  5.7828240e-02  1.2133410e-02\n  1.0294540e-02 -8.0481160e-02 -2.4606356e-01 -7.3678100e-02\n -8.4661840e-02  1.0843920e-01  5.5433200e-03  1.1077085e-01\n -5.1359450e-02 -1.4247670e-02  1.7330882e-01  1.7245873e-01\n  4.9515490e-02  6.0317260e-02 -1.0771478e-01  9.0432840e-02\n  1.2291482e-01  3.2809460e-02  3.6963750e-02  9.2618940e-02\n -6.5803100e-03  1.3897658e-01 -9.4685410e-02 -1.1382614e-01\n -3.0152520e-02  7.0715790e-02 -1.7909274e-01 -2.0787367e-01\n -9.4159370e-02 -1.3235682e-01  2.3704279e-01 -6.7392020e-02\n  9.4320480e-02  9.1353400e-03  2.5858670e-02 -4.7034110e-02\n -3.1155800e-03 -5.6337130e-02 -4.4191830e-02  5.8343700e-03\n -2.3297030e-02  5.2224510e-02 -9.9326370e-02 -1.1128379e-01\n  1.8970777e-01 -4.8372610e-02  1.1136933e-01 -1.8569734e-01\n -1.0684215e-01 -1.2544549e-01 -7.0083590e-02  4.3423220e-02\n -7.8571000e-03  2.8568420e-02 -1.2919581e-01  6.2937640e-02\n  1.5853259e-01 -1.0556887e-01 -2.1359038e-01 -2.0035470e-02\n -1.0639659e-01 -1.2102153e-01 -2.7586600e-02  8.9672650e-02\n -2.3192000e-02 -4.6934600e-03  7.6376500e-02  9.8112700e-03\n -8.3998690e-02 -8.5690120e-02 -5.4611250e-02  1.6400595e-01\n -2.3957000e-02  8.9056520e-02  1.4383290e-01 -6.2483770e-02\n -2.2609500e-02 -1.6693653e-01 -2.9105330e-02  9.2582230e-02\n -9.5289210e-02  1.9819485e-01 -6.7757080e-02 -8.1780490e-02\n  5.1958510e-02 -6.5862340e-02  1.0946754e-01  4.6133410e-02\n -1.1779869e-01  4.1433720e-02  1.5953435e-01  1.0682953e-01]\nbn:00019305n:[-2.8633840e-02  2.8483490e-02 -5.9769400e-03  1.5226870e-01\n -6.9004040e-02  2.5442000e-03  9.8903630e-02 -1.4648657e-01\n  2.8913200e-03  1.9954792e-01  7.0749260e-02 -9.6714450e-02\n -2.9596920e-02  7.4930460e-02 -2.1081796e-01  1.1591651e-01\n -1.7603108e-01  1.9631607e-01 -3.9172990e-02 -1.4909600e-02\n -2.1180610e-02  4.3154520e-02  6.6950770e-02 -7.3285000e-04\n -3.4381880e-02 -1.0720206e-01 -1.3589152e-01  7.5040030e-02\n -2.2683790e-02  4.1742130e-02 -1.4104230e-02 -7.7698000e-03\n  8.2722470e-02 -1.3960400e-03 -5.4560510e-02  5.0105760e-02\n -1.4641600e-03 -1.8629255e-01  7.9851730e-02  1.0895063e-01\n  1.1879093e-01 -1.2857992e-01  2.0858850e-02  3.3384700e-03\n -6.3669380e-02 -2.6606851e-01 -4.7380300e-02 -3.2326590e-02\n -7.0265280e-02  3.8858270e-02 -1.2263874e-01  3.7518770e-02\n  4.8875430e-02 -1.4547500e-03 -4.7980300e-03  4.9446520e-02\n -2.2475930e-02  4.4056700e-03 -4.5777950e-02 -1.1684404e-01\n -1.0572780e-01 -1.3128700e-03 -8.4947640e-02 -1.0983970e-02\n -5.2111910e-02 -1.0852704e-01 -3.3609190e-02 -1.5897204e-01\n  1.1750578e-01  1.3767060e-02  7.9842200e-02  5.4863300e-03\n -2.2479220e-02 -8.6354250e-02 -2.0698526e-01 -3.6085860e-02\n -3.1502730e-02 -7.4135670e-02 -1.0578750e-02  4.7131640e-02\n -6.8242850e-02  2.1355890e-02 -7.2628500e-03 -1.2809981e-01\n -2.2027740e-02  2.8853830e-02 -8.9262240e-02  2.0393856e-01\n -2.5222840e-02  9.2948880e-02 -3.4855240e-02 -1.0986327e-01\n -1.9449920e-02  1.0519870e-02 -8.5437600e-03 -1.4870940e-02\n  7.0641000e-03  1.1400030e-02  5.7802810e-02 -4.1957210e-02\n -1.2542614e-01 -7.7606210e-02  7.2819370e-02  2.2491800e-02\n -3.7816810e-02 -9.8577120e-02  7.3102500e-02  8.0426020e-02\n -1.6933115e-01 -1.6550339e-01 -2.1168634e-01 -1.6281791e-01\n  5.8043040e-02  4.3416540e-02  2.6021230e-02  4.5698320e-02\n  4.2402120e-02 -8.9890050e-02  1.5373960e-02 -3.7328360e-02\n -9.2739800e-03  6.5421410e-02 -2.3747760e-02 -3.6375270e-02\n  6.7305240e-02  8.0271580e-02 -9.7161840e-02  3.5841200e-02\n  1.4749087e-01  1.7287240e-01 -1.1838335e-01  1.2824530e-02\n  9.3832800e-03 -2.1609920e-02  1.9891010e-02  2.0029111e-01\n -2.2529690e-02  8.0235700e-03  1.0690772e-01 -3.0754110e-02\n -2.5748150e-02 -1.5254882e-01 -1.5699471e-01 -5.3195050e-02\n  1.0660307e-01  1.2762640e-02  1.0060488e-01  9.4808580e-02\n -2.8908010e-01 -7.7819000e-02  1.6357663e-01 -5.9704430e-02\n -1.6429031e-01  1.2381510e-01 -2.3456850e-02 -1.2785055e-01\n -7.9365590e-02 -1.4877326e-01 -1.0009185e-01  2.0069990e-02\n -1.2723519e-01 -3.7533150e-02  1.2103335e-01  6.8399960e-02\n  4.6344130e-02 -8.6578220e-02  1.2550712e-01 -3.9094000e-03\n -7.4390500e-02 -3.5736340e-02 -1.2644223e-01  9.2718910e-02\n -7.6809620e-02 -4.3345730e-02 -1.8386957e-01 -6.4859110e-02\n  1.4161586e-01 -1.1123453e-01 -3.1936200e-03  6.2377490e-02\n -1.4591703e-01 -1.0565197e-01 -1.0065474e-01  8.6243500e-02\n -1.3416395e-01 -6.7642140e-02 -3.9137620e-02  9.5400700e-03\n -6.5929600e-02  3.7589640e-02 -7.7712450e-02  6.4245240e-02\n -1.3459191e-01 -4.7466820e-02  1.4256820e-01  1.9304675e-01\n  1.7458160e-02 -4.9136360e-02 -2.8301610e-02 -1.4413204e-01\n  7.2851840e-02 -1.2355388e-01 -3.2451330e-02  3.4421500e-02\n -4.8592330e-02 -3.4573320e-02  3.1082920e-02  3.6004500e-03\n -1.2061500e-02 -1.1464580e-01 -2.7178006e-01 -5.5540410e-02\n -1.0127692e-01  1.4681884e-01 -1.2929100e-03  7.8565640e-02\n -3.4138770e-02 -9.3900000e-03  1.3911909e-01  1.9108708e-01\n  2.8354870e-02  5.1537200e-02 -9.3155910e-02  9.7211290e-02\n  1.0402600e-01  5.8022740e-02  1.4882470e-02  8.4537780e-02\n -2.1181100e-03  1.1200527e-01 -5.6917510e-02 -1.0058184e-01\n -1.9113410e-02  6.9339510e-02 -1.4170260e-01 -2.1173048e-01\n -8.5241740e-02 -1.0238703e-01  2.2046795e-01 -6.2734690e-02\n  1.0652473e-01 -2.6814000e-04  1.5466960e-02 -4.8180040e-02\n  1.7037170e-02 -4.6012020e-02 -4.9626230e-02  2.3315270e-02\n -1.5790330e-02  4.8690030e-02 -8.8161390e-02 -9.7490610e-02\n  1.7646038e-01 -2.3760100e-02  1.0778635e-01 -1.5569171e-01\n -1.2382513e-01 -1.4574776e-01 -5.0227690e-02  1.5728630e-02\n  2.9954860e-02  5.2084780e-02 -1.5203847e-01  4.9432660e-02\n  1.2341665e-01 -1.0102523e-01 -1.9171893e-01  1.6467880e-02\n -8.7241480e-02 -1.1159726e-01 -1.9517160e-02  8.3385400e-02\n  3.6163840e-02  1.8400020e-02  8.9797760e-02  1.4650750e-02\n -8.2854780e-02 -9.8796370e-02 -6.9015020e-02  1.2822964e-01\n -3.5298670e-02  7.8336900e-02  1.2832190e-01 -8.8926380e-02\n -3.6130770e-02 -1.5234138e-01 -2.9805520e-02  1.3067892e-01\n -1.0580153e-01  1.8861986e-01 -4.9091650e-02 -5.4688020e-02\n  4.0157240e-02 -1.1028260e-01  9.1920140e-02  2.4599110e-02\n -6.7019680e-02  1.3910920e-02  1.2823287e-01  1.2818750e-01]\nbn:15303858n:None\n"
     ]
    }
   ],
   "source": [
    "lemma = 'agrume'\n",
    "senses_id = nasari.get_lemma_senses(lemma) # just a wrapper to semeval mapper instance\n",
    "vectors = nasari.get_lemma_vectors(lemma)\n",
    "print(f\"Il lemma: '{lemma}' ha i seguenti possibili babel synsets: {senses_id}\\n\")\n",
    "\n",
    "print(\"Rappresentazione embedded:\")\n",
    "for sense_id, vector in zip(senses_id, vectors):\n",
    "    print(f\"{sense_id}:{vector}\")"
   ]
  },
  {
   "source": [
    "Da notare l'ultimo synset: la versione nasari utilizzata √® una versione ridotta, dunque √® possibile avere dei sensi senza il corrispettivo vettore associato!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task 1: Semantic Word Similarity\n",
    "\n",
    "Il task prevede nella prima parte l'annotazione manuale di coppie di parole (`file words_annotations.tsv`) con uno score di similarit√† compreso in $[0,4]$.\n",
    "L'annotazione √® stata effettuata seguendo gli stessi criteri utilizzati nel corpus *SemEval2017ITA*:\n",
    "\n",
    "* 4: Very similar -- The two words are synonyms (e.g., midday-noon).\n",
    "* 3: Similar -- The two words share many of the important ideas of their meaning but\n",
    "include slightly different details. They refer to similar but not identical concepts (e.g., lionzebra).\n",
    "* 2: Slightly similar -- The two words do not have a very similar meaning, but share a\n",
    "common topic/domain/function and ideas or concepts that are related (e.g., house-window).\n",
    "* 1: Dissimilar -- The two items describe clearly dissimilar concepts, but may share some\n",
    "small details, a far relationship or a domain in common and might be likely to be found\n",
    "together in a longer document on the same topic (e.g., software-keyboard).\n",
    "* 0: Totally dissimilar and unrelated -- The two items do not mean the same thing and are\n",
    "not on the same topic (e.g., pencil-frog\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             lemma1         lemma2  score\n",
       "0        recessione            PIL    2.5\n",
       "1            Cesare  Giulio Cesare    4.0\n",
       "2          paziente       sessione    3.0\n",
       "3  comportamentismo        terapia    2.9\n",
       "4        imperatore   costituzione    2.6"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemma1</th>\n      <th>lemma2</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>recessione</td>\n      <td>PIL</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cesare</td>\n      <td>Giulio Cesare</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>paziente</td>\n      <td>sessione</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>comportamentismo</td>\n      <td>terapia</td>\n      <td>2.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>imperatore</td>\n      <td>costituzione</td>\n      <td>2.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "annotations = pd.read_csv(Path('data/words_annotations.tsv'), sep='\\t')\n",
    "annotations.head()"
   ]
  },
  {
   "source": [
    "Una volta conclusa la fase di annotazione, gli score di similarit√† ottenuti rappresentano le annotazioni \"gold standard\" con cui possiamo confrontare i risultati ottenuti da un algoritmo. Nel caso specifico, per confrontare la similirit√† semantica utilizziamo la metrica *cosine similarity* su i vettori Nasari embedded.\n",
    "\n",
    "Dato il fenomeno della polisiam ad uno specifico lemma pu√≤ essere associato pi√π di un vettore, per questo motivo calcoliamo la massima similarit√† tra tutte le possibili coppie di sensi tra il primo e il secondo lemma della coppia, in formula:\n",
    "\n",
    "$$ \\operatorname{sim}(w_1, w_2) = \\operatorname*{max}_{\\substack{s_i \\in \\operatorname{Senses}(w_1)\\\\ s_j \\in \\operatorname{Senses}(w_2) }} \\operatorname{sim}(s_i,s_j)$$\n",
    "\n",
    "Nello nostro come funzione di simlarit√† viene utilizzata la similarit√† del coseno:\n",
    "\n",
    "$$ \\operatorname{sim}(s_i,s_j) = \\frac{s_i\\cdot s_j}{||s_i||\\cdot||s_j||}$$\n",
    "\n",
    "dove $s_i, s_j$ sono rispettivamente tutti i possibili sensi del lemma $w_1$ e $w_2$.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             lemma1         lemma2  score  system_score\n",
       "0        recessione            PIL    2.5      0.898526\n",
       "1            Cesare  Giulio Cesare    4.0      1.000000\n",
       "2          paziente       sessione    3.0      0.481492\n",
       "3  comportamentismo        terapia    2.9      0.629044\n",
       "4        imperatore   costituzione    2.6      0.635566"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemma1</th>\n      <th>lemma2</th>\n      <th>score</th>\n      <th>system_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>recessione</td>\n      <td>PIL</td>\n      <td>2.5</td>\n      <td>0.898526</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cesare</td>\n      <td>Giulio Cesare</td>\n      <td>4.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>paziente</td>\n      <td>sessione</td>\n      <td>3.0</td>\n      <td>0.481492</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>comportamentismo</td>\n      <td>terapia</td>\n      <td>2.9</td>\n      <td>0.629044</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>imperatore</td>\n      <td>costituzione</td>\n      <td>2.6</td>\n      <td>0.635566</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "scores = annotations.copy()\n",
    "scores['system_score'] = scores.apply(lambda x: sense_similarity_score(x['lemma1'], x['lemma2'], cosine_similarity, nasari), \n",
    "                                                axis=1)\n",
    "scores.head()"
   ]
  },
  {
   "source": [
    "Una volta ottenuti gli score per ogni coppia di lemmi, possiamo verificare la relazione che intercorre tra lo score gold standard e quello ottenuto dal sistema, utilizzando gli indici di correlazione di Pearson e Spearman"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6720570416841988"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "scores.corr(method='pearson').loc['score','system_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7003267480858936"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "scores.corr(method='spearman').loc['score','system_score']"
   ]
  },
  {
   "source": [
    "## Task 2\n",
    "\n",
    "Questo task consiste nell'annotare i termini con i rispettivi sensi e successivamente, seguendo la stessa metodologia del task 1,  valutare l'**accuratezza** del sistema rispetto alle annotazioni effettuate.\n",
    "\n",
    "\n",
    "Partendo dalla stessa lista di coppie di lemmi estratti nel task 1, il processo prevede di annotare ciascun lemma con il corrispettivo Babel synset ID e una lista di termini che costituiscono un contesto di disambiguazione."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             lemma1      senseID1  \\\n",
       "0        recessione  bn:00066516n   \n",
       "1            Cesare  bn:00014550n   \n",
       "2          paziente  bn:00001742n   \n",
       "3          paziente  bn:00061017n   \n",
       "4  comportamentismo  bn:00009659n   \n",
       "\n",
       "                                              terms1         lemma2  \\\n",
       "0  {'recessione', 'Depressione_economica', 'Reces...            PIL   \n",
       "1  {'Gaius_Julius_Caesar', 'Gaio_Giulio_Cesare', ...  Giulio Cesare   \n",
       "2  {'ruolo_del_paziente', 'paziente', 'ruolo_inte...       sessione   \n",
       "3  {'malato', 'pazienti_ricoverati', 'Pazienti', ...        terapia   \n",
       "4  {'comportamentisti', 'analisi_del_comportament...   costituzione   \n",
       "\n",
       "       senseID2                                             terms2  \n",
       "0  bn:00037570n  {'prodotto_Interno_Lordo', 'Prodotto_interno_l...  \n",
       "1  bn:00014550n  {'Gaius_Julius_Caesar', 'Gaio_Giulio_Cesare', ...  \n",
       "2  bn:00070690n                  {'tornata', 'sessione', 'seduta'}  \n",
       "3  bn:00076842n                         {'terapeutica', 'terapia'}  \n",
       "4  bn:00022052n  {'Costituzione_materiale', 'costituzione_codif...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemma1</th>\n      <th>senseID1</th>\n      <th>terms1</th>\n      <th>lemma2</th>\n      <th>senseID2</th>\n      <th>terms2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>recessione</td>\n      <td>bn:00066516n</td>\n      <td>{'recessione', 'Depressione_economica', 'Reces...</td>\n      <td>PIL</td>\n      <td>bn:00037570n</td>\n      <td>{'prodotto_Interno_Lordo', 'Prodotto_interno_l...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cesare</td>\n      <td>bn:00014550n</td>\n      <td>{'Gaius_Julius_Caesar', 'Gaio_Giulio_Cesare', ...</td>\n      <td>Giulio Cesare</td>\n      <td>bn:00014550n</td>\n      <td>{'Gaius_Julius_Caesar', 'Gaio_Giulio_Cesare', ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>paziente</td>\n      <td>bn:00001742n</td>\n      <td>{'ruolo_del_paziente', 'paziente', 'ruolo_inte...</td>\n      <td>sessione</td>\n      <td>bn:00070690n</td>\n      <td>{'tornata', 'sessione', 'seduta'}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>paziente</td>\n      <td>bn:00061017n</td>\n      <td>{'malato', 'pazienti_ricoverati', 'Pazienti', ...</td>\n      <td>terapia</td>\n      <td>bn:00076842n</td>\n      <td>{'terapeutica', 'terapia'}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>comportamentismo</td>\n      <td>bn:00009659n</td>\n      <td>{'comportamentisti', 'analisi_del_comportament...</td>\n      <td>costituzione</td>\n      <td>bn:00022052n</td>\n      <td>{'Costituzione_materiale', 'costituzione_codif...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "senses_annotations = pd.read_csv(Path('data/senses_annotations.tsv'), sep='\\t')\n",
    "senses_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "babelnet = dm.BabelNet(os.environ['BABELNET_KEY'])\n",
    "\n",
    "words1 = pd.DataFrame(annotations['word1'])\n",
    "words1['babelID'] = words1.apply(lambda x: list(semeval.get_synsets(x['word1'])), axis=1)\n",
    "words1 = words1.explode('babelID', ignore_index=True)\n",
    "\n",
    "words1['lemmas'] = words1.apply(lambda x: babelnet.get_synset_lemmas(x['babelID']), axis=1)\n",
    "\n",
    "words1.to_csv('output/word1')\n",
    "\n",
    "\n",
    "words2 = pd.DataFrame(annotations['word2'])\n",
    "words2['babelID'] = words2.apply(lambda x: list(semeval.get_synsets(x['word2'])), axis=1)\n",
    "words2 = words2.explode('babelID', ignore_index=True)\n",
    "\n",
    "words2['lemmas'] = words2.apply(lambda x: babelnet.get_synset_lemmas(x['babelID']), axis=1)\n",
    "\n",
    "words1.to_pickle('output/words1_df.pkl')\n",
    "words2.to_pickle('output/words2_df.pkl')\n",
    "\n",
    "words1 = pd.read_pickle(Path('output/words1_df.pkl'))\n",
    "words2 = pd.read_pickle(Path('output/words2_df.pkl'))\n",
    "\n",
    "senses1 = pd.read_csv('output/words1', sep=',', index_col=0, header=0, names=['lemma1','senseID1','terms1']).reset_index(drop=True)\n",
    "senses2 = pd.read_csv('output/words2', sep=',', index_col=0, header=0, names=['lemma2','senseID2','terms2']).reset_index(drop=True)\n",
    "senses = pd.concat([senses1,senses2], axis=1)\n",
    "senses.to_csv('output/senses_annotations.tsv', sep='\\t')\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses_scores = senses_annotations[['lemma1','lemma2',\n",
    "                                    'senseID1','senseID2']].copy() # discard terms list columns\n",
    "\n",
    "senses_scores['system_senseID1'] = senses_scores.apply(lambda x: sense_similarity(x['lemma1'], x['lemma2'], cosine_similarity, nasari)[1], \n",
    "                                                        axis=1) # [1] take the sense of the first lemma\n",
    "senses_scores['system_senseID2'] = senses_scores.apply(lambda x: sense_similarity(x['lemma1'], x['lemma2'], cosine_similarity, nasari)[2], \n",
    "                                                        axis=1) # [2] take the sense of the second lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             lemma1         lemma2      senseID1      senseID2  \\\n",
       "0        recessione            PIL  bn:00066516n  bn:00037570n   \n",
       "1            Cesare  Giulio Cesare  bn:00014550n  bn:00014550n   \n",
       "2          paziente       sessione  bn:00001742n  bn:00070690n   \n",
       "3          paziente        terapia  bn:00061017n  bn:00076842n   \n",
       "4  comportamentismo   costituzione  bn:00009659n  bn:00022052n   \n",
       "\n",
       "  system_senseID1 system_senseID2  \n",
       "0    bn:00066516n    bn:00037570n  \n",
       "1    bn:00014550n    bn:00014550n  \n",
       "2    bn:00001742n    bn:03751534n  \n",
       "3    bn:00061017n    bn:00076842n  \n",
       "4    bn:00009659n    bn:00059480n  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemma1</th>\n      <th>lemma2</th>\n      <th>senseID1</th>\n      <th>senseID2</th>\n      <th>system_senseID1</th>\n      <th>system_senseID2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>recessione</td>\n      <td>PIL</td>\n      <td>bn:00066516n</td>\n      <td>bn:00037570n</td>\n      <td>bn:00066516n</td>\n      <td>bn:00037570n</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cesare</td>\n      <td>Giulio Cesare</td>\n      <td>bn:00014550n</td>\n      <td>bn:00014550n</td>\n      <td>bn:00014550n</td>\n      <td>bn:00014550n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>paziente</td>\n      <td>sessione</td>\n      <td>bn:00001742n</td>\n      <td>bn:00070690n</td>\n      <td>bn:00001742n</td>\n      <td>bn:03751534n</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>paziente</td>\n      <td>terapia</td>\n      <td>bn:00061017n</td>\n      <td>bn:00076842n</td>\n      <td>bn:00061017n</td>\n      <td>bn:00076842n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>comportamentismo</td>\n      <td>costituzione</td>\n      <td>bn:00009659n</td>\n      <td>bn:00022052n</td>\n      <td>bn:00009659n</td>\n      <td>bn:00059480n</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "senses_scores.head()"
   ]
  },
  {
   "source": [
    "Come si pu√≤ notare dall'output, in 4 casi su i 5 mostrati il sistema predice lo stesso senso per il lemma 1, per il lemma 2 solo in 3 cas su 5.\n",
    "\n",
    "Analizziamo ora l'accuracy totale considerando i singoli lemmi e rispetto alla coppia:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, predicted):\n",
    "    correct_predictions = sum([true_sense.lower() == predicted_sense.lower() for true_sense, predicted_sense in zip(true, predicted)\n",
    "    if true_sense and predicted_sense])\n",
    "    return correct_predictions / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuratezza sui singoli lemmi: 0.46\n"
     ]
    }
   ],
   "source": [
    "lemmas_gold_senses = pd.concat([senses_scores['senseID1'], senses_scores['senseID2']], \n",
    "                               axis=0, ignore_index=True) # concat lemmas vertically\n",
    "\n",
    "lemmas_system_senses = pd.concat([senses_scores['system_senseID1'], senses_scores['system_senseID2']],\n",
    "                                axis=0, ignore_index=True) # concat lemmas vertically\n",
    "accuracy_score = accuracy(lemmas_gold_senses, lemmas_system_senses)\n",
    "\n",
    "print(f\"Accuratezza sui singoli lemmi: {accuracy_score}\")"
   ]
  },
  {
   "source": [
    "Per valutare l'accuracy della coppia utilizziamo un semplice trick, concateniamo i due ysnset dei singoli lemmi e valutiamo l'accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuratezza sulla coppia di lemmi: 0.3\n"
     ]
    }
   ],
   "source": [
    "pairs_gold_senses = senses_scores['senseID1']+senses_scores['senseID2']\n",
    "pairs_system_senses = senses_scores['system_senseID1'].str.cat(senses_scores['system_senseID2'], na_rep='None')\n",
    "\n",
    "accuracy_score = accuracy(gold_senses_pairs, system_senses_pairs)\n",
    "\n",
    "print(f\"Accuratezza sulla coppia di lemmi: {accuracy_score}\")"
   ]
  },
  {
   "source": [
    "## Risultati\n",
    "\n",
    "In entrambi i task abbiamo cercato di stabilire la similarit√† semantica tra coppie di termini. Sebbene i task utilizzano lo stesso metodo, il loro obbiettivo √® differente. Nel primo caso bisgna solo quantificare la similarit√† a livello semantico di due termini; il secondo √® nettamente pi√π difficile in quanto bisogna individuare in maniera puntuale il senso di un termine.\n",
    "\n",
    "La forte correlazione ottenuta nel primo task ($\\approx 0.67$ Pearson, $\\approx 0.7$ Spearman) non deve trarre in inganno perch√© non indica che il sistema abbia individuato il senso corretto, infatti a supporto di questa osservazione, possiamo considerare i bassi valori di accuracy ottenuta $0.46$.\n",
    "\n",
    "Con poca sorpresa, l'accuracy ottenuta considerando i lemmi congiuntamente si abbassa ulteriormente a $0.3$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}