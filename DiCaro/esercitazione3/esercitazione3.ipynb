{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd000c4925ce5d24b744b1d4673b76423a641f8c01c448e9592271d32b0e3be0330",
   "display_name": "Python 3.9.4 64-bit ('TLN': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Esercitazione 3: Teoria di Hanks\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO\n",
    "* Recuperare da un corpus n istanze in cui esso viene usato XXX\n",
    "\n",
    "* Effettuare parsing e disambiguazione\n",
    "\n",
    "* Usare i super sensi di WordNet sugli argomenti (subj e obj) del verbo scelto\n",
    "\n",
    "* Aggregare i risultati, calcolare le frequenze, stampare i cluster semantici ottenuti"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package brown to /home/prf/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VERB = 'eat'\n",
    "\n",
    "VERB_POS_TAGS = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# stanford parser dependencies types\n",
    "SUBJ_DEP_TYPES = ['nsubj', 'nsubjpass']\n",
    "OBJ_DEP_TYPES = ['dobj', 'iobj']\n",
    "\n",
    "VALENCE = 2"
   ]
  },
  {
   "source": [
    "### Step 1: estrazione frasi contenti verbo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def corpus_extraction(verb, corpus,  verb_pos_tags = VERB_POS_TAGS):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = corpus.sents()\n",
    "\n",
    "    selected_sentences = []\n",
    "    for sent in sentences:\n",
    "        tags = dict(nltk.pos_tag(sent))\n",
    "        for word in sent:\n",
    "            if tags[word] in VERB_POS_TAGS: # extract only verbs\n",
    "                word = lemmatizer.lemmatize(word, 'v')\n",
    "                if word == verb: # extract only sentences with the given verb\n",
    "                    selected_sentences.append(sent)\n",
    "\n",
    "    return selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 extract sentences\n",
    "selected_sents = corpus_extraction(INPUT_VERB, brown)"
   ]
  },
  {
   "source": [
    "# Step 2: Estrazione Fillers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import namedtuple\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dep_allowed_types = OBJ_DEP_TYPES + SUBJ_DEP_TYPES\n",
    "def find_target_verb(doc, verb, verb_pos_tags = VERB_POS_TAGS):\n",
    "    targets = []\n",
    "    for token in doc:\n",
    "        # search for verbs first and also in the lemmatized form\n",
    "        if token.tag_ in verb_pos_tags and (token.text == verb or \n",
    "                                            token.lemma_ == verb):\n",
    "            targets.append(token)\n",
    "    return targets\n",
    "\n",
    "\n",
    "def get_hanks_verb(verb_token, dep_allowed_types=OBJ_DEP_TYPES + SUBJ_DEP_TYPES):\n",
    "    slot_values = []\n",
    "    filler_values = []\n",
    "    \n",
    "    for children in verb_token.children:\n",
    "        if children.dep_ in dep_allowed_types:\n",
    "            slot_values.append(children.dep_) \n",
    "            filler_values.append(children.text)\n",
    "\n",
    "    # dynamically create a named tuple based on number of slot/fillers found\n",
    "    slot_names = [f\"slot{i}\" for i,_ in enumerate(slot_values, start=1)]\n",
    "    filler_names = [f\"filler{i}\" for i,_ in enumerate(filler_values, start=1)]\n",
    "    \n",
    "    attrib_names = ['verb', 'nargs'] + slot_names + filler_names\n",
    "    attrib_values = [verb_token.text, len(slot_names)] + slot_values + filler_values\n",
    "\n",
    "    HanksVerb = namedtuple(\"HanksVerb\", attrib_names)\n",
    "\n",
    "    return HanksVerb(*attrib_values)\n",
    "\n",
    "def find_verb_fillers(sentences, verb, valence=VALENCE):\n",
    "    fillers = []\n",
    "\n",
    "    # this tokenization step is necessary for Spacy since the pipeline takes a string as input\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    joined_sents = [detokenizer.detokenize(sent) for sent in sentences]\n",
    "\n",
    "    for sentence, joined_sent in zip(sentences,joined_sents):\n",
    "        doc = nlp(joined_sent)\n",
    "        \n",
    "        # find target verb occurences in the sentence\n",
    "        target_verbs = find_target_verb(doc, verb)\n",
    "        \n",
    "        # loop trough target verb occurences\n",
    "        for target_verb in target_verbs:\n",
    "            \n",
    "            # retrieve verb fillers from syntactic dependencies\n",
    "            hank_verb = get_hanks_verb(target_verb)\n",
    "            # check for valence\n",
    "            if hank_verb.nargs == valence:\n",
    "                fillers.append((sentence, hank_verb))\n",
    "    return fillers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 find fillers\n",
    "fillers = find_verb_fillers(selected_sents, INPUT_VERB, valence=2)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Step 3: WSD & sense clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.wsd as wsd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def find_filler_senses(fillers, wsd_func):\n",
    "    filler_senses = []\n",
    "    \n",
    "    for sentence, hanks_verb in fillers:\n",
    "        filler1_sense = wsd_func(sentence, hanks_verb.filler1)\n",
    "        filler2_sense = wsd_func(sentence, hanks_verb.filler2)\n",
    "\n",
    "        filler_senses.append((filler1_sense, filler2_sense))\n",
    "    \n",
    "    return filler_senses\n",
    "\n",
    "def semantic_clustering(filler_senses):\n",
    "    semantic_types = Counter()\n",
    "\n",
    "    for filler1_sense, filler2_sense in filler_senses:\n",
    "        if filler1_sense is not None and filler2_sense is not None:\n",
    "                # implicit clustering with wn supersenses\n",
    "                semantic_type = (filler1_sense.lexname(), filler2_sense.lexname())\n",
    "                # keep track of semantic_type occurrences\n",
    "                semantic_types.update([semantic_type])\n",
    "        else:\n",
    "            semantic_types.update([(None, None)]) # just to keep track of invalid semantic type senses\n",
    "        \n",
    "    return semantic_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_senses = find_filler_senses(fillers, wsd.lesk)\n",
    "semantic_types = semantic_clustering(filler_senses)"
   ]
  },
  {
   "source": [
    "### Valutazione"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Applying Hanks Theory for verb: eat\nfiller: what - people\nsenses: None - Synset('citizenry.n.01')\n-------------------------------------------------------------\nfiller: Americans - fat\nsenses: Synset('american_english.n.01') - Synset('fatty.a.01')\n-------------------------------------------------------------\nfiller: one - skin\nsenses: Synset('one.s.03') - Synset('skin.v.02')\n-------------------------------------------------------------\nfiller: what - they\nsenses: None - None\n-------------------------------------------------------------\nfiller: students - cereal\nsenses: Synset('scholar.n.01') - Synset('grain.n.02')\n-------------------------------------------------------------\nfiller: they - it\nsenses: None - Synset('information_technology.n.01')\n-------------------------------------------------------------\nfiller: students - cereal\nsenses: Synset('scholar.n.01') - Synset('grain.n.02')\n-------------------------------------------------------------\nfiller: they - it\nsenses: None - Synset('information_technology.n.01')\n-------------------------------------------------------------\nfiller: I - rider\nsenses: Synset('one.n.01') - Synset('rider.n.01')\n-------------------------------------------------------------\nfiller: I - eggs\nsenses: Synset('iodine.n.01') - Synset('testis.n.01')\n-------------------------------------------------------------\nfiller: we - seed\nsenses: None - Synset('seeded_player.n.01')\n-------------------------------------------------------------\nfiller: which - profits\nsenses: None - Synset('net_income.n.01')\n-------------------------------------------------------------\nfiller: who - celery\nsenses: Synset('world_health_organization.n.01') - Synset('celery.n.01')\n-------------------------------------------------------------\nfiller: I - soup\nsenses: Synset('one.s.01') - Synset('soup.n.02')\n-------------------------------------------------------------\nfiller: They - food\nsenses: None - Synset('food.n.02')\n-------------------------------------------------------------\nfiller: I - more\nsenses: Synset('one.s.01') - Synset('more.r.02')\n-------------------------------------------------------------\nfiller: Charlie - supper\nsenses: None - Synset('supper.n.01')\n-------------------------------------------------------------\nfiller: she - dinner\nsenses: None - Synset('dinner.n.01')\n-------------------------------------------------------------\nfiller: you - it\nsenses: None - Synset('information_technology.n.01')\n-------------------------------------------------------------\nfiller: fish - bodies\nsenses: Synset('fish.n.02') - Synset('torso.n.01')\n-------------------------------------------------------------\nfiller: what - I\nsenses: None - Synset('one.s.01')\n-------------------------------------------------------------\nfiller: you - dust\nsenses: None - Synset('dust.v.02')\n-------------------------------------------------------------\nfiller: I - this\nsenses: Synset('one.n.01') - None\n-------------------------------------------------------------\nfiller: He - breakfast\nsenses: Synset('helium.n.01') - Synset('breakfast.v.02')\n-------------------------------------------------------------\nfiller: they - other\nsenses: None - Synset('other.s.04')\n-------------------------------------------------------------\nfiller: They - breakfast\nsenses: None - Synset('breakfast.v.02')\n-------------------------------------------------------------\nfiller: She - bread\nsenses: None - Synset('bread.v.01')\n-------------------------------------------------------------\nfiller: They - chickens\nsenses: None - Synset('chicken.n.02')\n-------------------------------------------------------------\nfiller: they - chickens\nsenses: None - Synset('wimp.n.01')\n-------------------------------------------------------------\n\nSemantic Types Clustering Informations for eat\nsemantic type: (None, None), abs freq:18, rel freq: 62.07%\nsemantic type: ('noun.person', 'noun.food'), abs freq:2, rel freq: 6.90%\nsemantic type: ('noun.communication', 'adj.all'), abs freq:1, rel freq: 3.45%\nsemantic type: ('adj.all', 'verb.contact'), abs freq:1, rel freq: 3.45%\nsemantic type: ('noun.quantity', 'noun.person'), abs freq:1, rel freq: 3.45%\nsemantic type: ('noun.substance', 'noun.body'), abs freq:1, rel freq: 3.45%\nsemantic type: ('noun.group', 'noun.plant'), abs freq:1, rel freq: 3.45%\nsemantic type: ('adj.all', 'noun.substance'), abs freq:1, rel freq: 3.45%\nsemantic type: ('adj.all', 'adv.all'), abs freq:1, rel freq: 3.45%\nsemantic type: ('noun.food', 'noun.body'), abs freq:1, rel freq: 3.45%\nsemantic type: ('noun.substance', 'verb.consumption'), abs freq:1, rel freq: 3.45%\n"
     ]
    }
   ],
   "source": [
    "def show_results(verb_name, fillers, filler_senses, semantic_types):\n",
    "    print(f\"Applying Hanks Theory for verb: {verb_name}\")\n",
    "    \n",
    "    for  (_, hanks_verb), (filler1_sense, filler2_sense) in zip(fillers, filler_senses):\n",
    "        print(f\"filler: {hanks_verb.filler1} - {hanks_verb.filler2}\")\n",
    "        print(f\"senses: {filler1_sense} - {filler2_sense}\")\n",
    "        print('-------------------------------------------------------------')\n",
    "    print(\"\")\n",
    "    print(f\"Semantic Types Clustering Informations for {verb_name}\")\n",
    "    tot = sum([freq for semantic_type, freq in semantic_types.most_common()])\n",
    "    for semantic_type, freq in semantic_types.most_common():\n",
    "        relative_freq = freq / tot * 100\n",
    "        print(f\"semantic type: {semantic_type}, abs freq:{freq}, rel freq: {relative_freq:0.2f}%\")\n",
    "\n",
    "show_results(INPUT_VERB, fillers, filler_senses, semantic_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(None, None)]"
      ]
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "list(semantic_types.elements())"
   ]
  },
  {
   "source": [
    "### Risultati\n",
    "\n",
    "* spesso i filler corrispondenti all'argomento subject del verbo corrispondono a pronomi (I, she, they, ecc...) i quali non trovano una corrispondente lexical category in wordnet dunque lesk ritorna NONE e non è possibile associare un semantic_type valido\n",
    "\n",
    "* cose si può notare l'invalid semantic type rappresenta il 68%, indicativo di come sia lo step di WSD che il coverage del sense repository (WN) sia fondamentale per il risultato finale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}