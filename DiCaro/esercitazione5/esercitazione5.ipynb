{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Superhero Name Generator - Learner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python394jvsc74a57bd000c4925ce5d24b744b1d4673b76423a641f8c01c448e9592271d32b0e3be0330",
      "display_name": "Python 3.9.4 64-bit ('TLN': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21XXWP7Fpt2L"
      },
      "source": [
        "# Esercitazione 5: Language Model con RNN\n",
        "\n",
        "In questa esercitazione vedremo come utilizzare un **language model** basato su RNN per generare nomi di supereroi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6P0NU5Cpt2R"
      },
      "source": [
        "### Dati\n",
        " Come primo step analizzamo i dati. Come sorgente di dati per il training della RNN è stato utilizzato il [Superhero Names Dataset](https://github.com/am1tyadav/superhero), una dataset contenente una lista di 9000+ nomi di supereroi e supercattivi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq4CLmsLpt2P"
      },
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "BEGIN_NAME_TOKEN= '$'\n",
        "END_NAME_TOKEN='\\t'\n",
        "PAD_CODE = 0\n",
        "\n",
        "with Path('data/superheroes.txt').open() as file:\n",
        "  data = file.readlines()\n",
        "\n",
        "data[:5]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jumpa\\t\\n', 'doctor fate\\t\\n', 'starlight\\t\\n', 'isildur\\t\\n', 'lasher\\t\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHeWW2aYFfZ2"
      },
      "source": [
        "Come si può notare dall'output, in ogni riga abbiamo un nome delimitato dal token \"\\t\" per indicare la fine della stringa. Manca però il token che delimita l'**inizio** del nome, dunque effettuiamo un breve processamento del file per aggiungere anche il carattere di inizio stringa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41pNTAZxvOtw"
      },
      "source": [
        "data_with_begin_token = [f\"{BEGIN_NAME_TOKEN}{line}\" for line in data] # add begin token\n",
        "\n",
        "with Path('data/superheroes_preprocessed.txt').open('w') as file:\n",
        "  for line in data_with_begin_token:\n",
        "    file.writelines(line)\n",
        "\n",
        "with Path('data/superheroes_preprocessed.txt').open() as file:\n",
        "  data = file.read()\n",
        "  \n",
        "data.splitlines()[:5]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$jumpa\\t', '$doctor fate\\t', '$starlight\\t', '$isildur\\t', '$lasher\\t']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXt1P1CJPD5t"
      },
      "source": [
        "### Build a char level vocabulary\n",
        "\n",
        "La generazione dei nomi avviene carattere-per-carattere, dunque dobbiamo tokenizzare i dati in input a livello di carattere e costruire un vocabolario che rappresenta un **indice** per i token individuati.\n",
        "\n",
        "Inoltre le reti NN prendono in input solo dati numerici, dobbiamo dunque associare ad ogni carattere del vocabolario un indice intero. Per tenere traccia del mapping tra caratteri e indici numerici, utilizzamo una semplice namedtuple come struttura dati: `Vocabulary`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Vocabulary = namedtuple(\"Vocabulary\", ['char_to_index', 'index_to_char', 'size'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Lo1Yqzpt2T"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    filters='!\"#%&()*+,-.:;/<=>?@[\\\\]^_`{|}~', # remove punctuations but retain $, \\t and \\n\n",
        "    split='\\n',\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "vocab = Vocabulary(char_to_index = tokenizer.word_index, \n",
        "                   index_to_char = tokenizer.index_word,\n",
        "                   size=len(tokenizer.index_word))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PQfcqG-weo9",
        "outputId": "908f2ef4-ec28-427d-d1d0-dc91084ff140"
      },
      "source": [
        "for char_entry, idx_entry in zip(vocab.char_to_index.items(), vocab.index_to_char.items()):\n",
        "    print(f\"{char_entry} <----> {idx_entry}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('$', 1) <----> (1, '$')\n('\\t', 2) <----> (2, '\\t')\n('a', 3) <----> (3, 'a')\n('e', 4) <----> (4, 'e')\n('r', 5) <----> (5, 'r')\n('o', 6) <----> (6, 'o')\n('n', 7) <----> (7, 'n')\n('i', 8) <----> (8, 'i')\n(' ', 9) <----> (9, ' ')\n('t', 10) <----> (10, 't')\n('s', 11) <----> (11, 's')\n('l', 12) <----> (12, 'l')\n('m', 13) <----> (13, 'm')\n('h', 14) <----> (14, 'h')\n('d', 15) <----> (15, 'd')\n('c', 16) <----> (16, 'c')\n('u', 17) <----> (17, 'u')\n('g', 18) <----> (18, 'g')\n('k', 19) <----> (19, 'k')\n('b', 20) <----> (20, 'b')\n('p', 21) <----> (21, 'p')\n('y', 22) <----> (22, 'y')\n('w', 23) <----> (23, 'w')\n('f', 24) <----> (24, 'f')\n('v', 25) <----> (25, 'v')\n('j', 26) <----> (26, 'j')\n('z', 27) <----> (27, 'z')\n('x', 28) <----> (28, 'x')\n('q', 29) <----> (29, 'q')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMwtIw_Bpt2Z"
      },
      "source": [
        "Sebbene la generazione dei nomi avviene a livello di singoli caratteri, l'input della rete sono sequenze di indici (che rappresentano caratteri).\n",
        "\n",
        "Le funzioni `name_to_seq` e `seq_to_name`, effettuano questa conversione da stringa di caratteri a sequenza di indici, e viceversa. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_-TTfqipt2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64785193-2529-48db-b2d2-5504973f5841"
      },
      "source": [
        "def name_to_seq(name, tokenizer):\n",
        "  return np.array(tokenizer.texts_to_sequences(name)).ravel() # flatten list of list into list of int\n",
        "\n",
        "def seq_to_name(seq, tokenizer):\n",
        "    return ''.join([tokenizer.index_word[idx] for idx in seq if idx != 0])\n",
        "\n",
        "for name in data.splitlines()[:10]:\n",
        "  seq = name_to_seq(name, tokenizer)\n",
        "  name_from_seq = seq_to_name(seq, tokenizer)\n",
        "  print(f\"{name} --> {seq} --> {name_from_seq}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$jumpa\t --> [ 1 26 17 13 21  3  2] --> $jumpa\t\n$doctor fate\t --> [ 1 15  6 16 10  6  5  9 24  3 10  4  2] --> $doctor fate\t\n$starlight\t --> [ 1 11 10  3  5 12  8 18 14 10  2] --> $starlight\t\n$isildur\t --> [ 1  8 11  8 12 15 17  5  2] --> $isildur\t\n$lasher\t --> [ 1 12  3 11 14  4  5  2] --> $lasher\t\n$varvara\t --> [ 1 25  3  5 25  3  5  3  2] --> $varvara\t\n$the target\t --> [ 1 10 14  4  9 10  3  5 18  4 10  2] --> $the target\t\n$axel\t --> [ 1  3 28  4 12  2] --> $axel\t\n$battra\t --> [ 1 20  3 10 10  5  3  2] --> $battra\t\n$changeling\t --> [ 1 16 14  3  7 18  4 12  8  7 18  2] --> $changeling\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gawCXZaTFSt"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCbAzsNjpt2m"
      },
      "source": [
        "### Preparazione del Dataset di train\n",
        "Come già accennato in precedenza, l'input della rete neurale è una sequenza e l'output un distribuzione di probabilità (sul vocabolario) **condizionata sulla sequenza in input**.\n",
        "\n",
        "Di seguito generiamo sequenze incrementeali che costituiranno l'input della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zstNn-0dpt2m"
      },
      "source": [
        "sequences = []\n",
        "names = data.splitlines()\n",
        "for name in names:\n",
        "    seq = name_to_seq(name, tokenizer)\n",
        "    if len(seq) >= 2: # minimal length for a seq\n",
        "      # build new seq incrementtaly shifting to the right one by one\n",
        "      sequences += [seq[:i] for i in range(2, len(seq) +1)] "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiNUbOLpXZQ-",
        "outputId": "4713f702-f0f9-494c-f28e-d33bac1d1b9c"
      },
      "source": [
        "for sequence in sequences[:15]:\n",
        "  name = seq_to_name(sequence, tokenizer)\n",
        "  print(f\"{sequence} --> {name}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1 26] --> $j\n[ 1 26 17] --> $ju\n[ 1 26 17 13] --> $jum\n[ 1 26 17 13 21] --> $jump\n[ 1 26 17 13 21  3] --> $jumpa\n[ 1 26 17 13 21  3  2] --> $jumpa\t\n[ 1 15] --> $d\n[ 1 15  6] --> $do\n[ 1 15  6 16] --> $doc\n[ 1 15  6 16 10] --> $doct\n[ 1 15  6 16 10  6] --> $docto\n[ 1 15  6 16 10  6  5] --> $doctor\n[ 1 15  6 16 10  6  5  9] --> $doctor \n[ 1 15  6 16 10  6  5  9 24] --> $doctor f\n[ 1 15  6 16 10  6  5  9 24  3] --> $doctor fa\n"
          ]
        }
      ]
    },
    {
      "source": [
        "Il problema principale nell'aver generato le sequenze in questo modo è che la rete accetta solo **input a dimensione fissa**. Dunque dobbiamo effettuare il **padding** delle sequenze in modo tale da rendere la lunghezza delle sequenze uniforme."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR68pu2tpt2q"
      },
      "source": [
        "max_seq_len = max([len(seq) for seq in sequences]) # needed for padding\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_seq_len, value=PAD_CODE)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPlrLRpSpt2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd78e28-f61d-4a5b-83b3-444b621022ef"
      },
      "source": [
        "padded_sequences.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97332, 34)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "source": [
        "Come si può notare la lunghezza delle sequenze è costante (34).\n",
        "\n",
        "Ultimo step è quello di dividere il dataset in train e test set. Prima però, ricordiamo che siamo in un contesto di **supervised learning** e dunque necessitiamo di **labels** da associare all'input in modo tale che la rete possa calcolare la *loss function**.\n",
        "\n",
        "Per soddisfare questo requisito, utilizziamo una strategia chiamata **Teacher Forcing**. "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE4BIeSnpt2v"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = (padded_sequences[:,:-1], # exclude last char of the sequence\n",
        "       padded_sequences[:,-1]) # the label is just the last char \n",
        "       \n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=1990)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfV344qAJqp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d960fb1-2e05-4504-ff62-d2f61576e6d7"
      },
      "source": [
        "print(f\"Dataset shape: {X.shape}, labels shape {y.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (97332, 33), labels shape (97332,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44DBptv7Jqp_"
      },
      "source": [
        "### Specifica dell'architettura della RNN\n",
        "\n",
        "L'architettura scelta è molto semplice, un livello di **embdedding** che mappa ogni token del vocabolario in uno spazio vettoriale multidimensionale ed un **livello hidden LSTM**. Il livello di output ha una funzione di attivazioe *+soft-max** che restituisce una distribuzione di probabilità sull'intero vocabolario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Ssl4qupt22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b8dd53-b4b4-42c7-f135-2508050d615c"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "    'vocab_size': vocab.size + 1 , # +1 is due to the padding char\n",
        "    'embedding_size': 16,\n",
        "    'seq_len': max_seq_len -1, # -1 since we split the last char of the seq for the label\n",
        "    'keep_prob': 0.7,\n",
        "    'hidden_units': 64,\n",
        "    'unroll': True,\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'optimizer': 'adam',\n",
        "    'batch_size': 128,\n",
        "    'epochs': 100,\n",
        "}\n",
        "HyperParams = namedtuple(\"hyperparameters\", hyperparams)\n",
        "HP = HyperParams(**hyperparams)\n",
        "\n",
        "\n",
        "model = Sequential(name='LM-RNN')\n",
        "\n",
        "model.add(Embedding(HP.vocab_size, HP.embedding_size, input_length=HP.seq_len))\n",
        "model.add(Dropout(1-HP.keep_prob))\n",
        "model.add(LSTM(HP.hidden_units))\n",
        "model.add(Dense(HP.vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss=HP.loss, optimizer=HP.optimizer, \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"LM-RNN\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 33, 16)            480       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 33, 16)            0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 64)                20736     \n_________________________________________________________________\ndense_1 (Dense)              (None, 30)                1950      \n=================================================================\nTotal params: 23,166\nTrainable params: 23,166\nNon-trainable params: 0\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHtg5f4nJqqA"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulVrrjfSHCD"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "\n",
        "class MakeNamesCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, tokenizer, n_names=10):\n",
        "      super().__init__()\n",
        "      self.tokenizer = tokenizer\n",
        "      self.n_names = n_names\n",
        " \n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "\n",
        "        for i in range(0, self.n_names):\n",
        "          name = self.generate_name(self.tokenizer, self.model)\n",
        "          print(f\"Generated name {name}\")\n",
        "\n",
        "    def generate_name(self, tokenizer, model, seed=BEGIN_NAME_TOKEN):\n",
        "        \"\"\"greedy search\"\"\"\n",
        "        name = seed\n",
        "        for i in range(0, 40):\n",
        "          seq = name_to_seq(name, tokenizer)\n",
        "          padded_seq = tf.keras.preprocessing.sequence.pad_sequences([seq], padding='pre',\n",
        "                                                                    maxlen=max_seq_len-1,value=PAD_CODE)\n",
        "        \n",
        "          prediction = model.predict(padded_seq)[0]\n",
        "          p = prediction[1:]/prediction[1:].sum()\n",
        "          index_vocab = list(tokenizer.index_word.keys())\n",
        "          best_idx = np.random.choice(index_vocab, p=p) # the 0-index is the softmax output for the pad character\n",
        "          best_char = tokenizer.index_word[best_idx]\n",
        "\n",
        "          name += best_char\n",
        "\n",
        "          if best_char == END_NAME_TOKEN:\n",
        "            break\n",
        "        return name"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f50aTRcpt24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b472c8-dcb5-484d-948d-f948c6dcce95"
      },
      "source": [
        "history = model.fit(X_train, y_train, \n",
        "          batch_size=HP.batch_size, epochs=HP.epochs,\n",
        "          validation_data=(X_test, y_test), verbose=2,\n",
        "          callbacks=[MakeNamesCallback(tokenizer=tokenizer)])\n",
        "\n",
        "#model.save(f\"{model.name}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Generated name $sunver gene\t\n",
            "Generated name $kirg dom\t\n",
            "Generated name $ietron\t\n",
            "Generated name $rei cones speersincer\t\n",
            "Generated name $han doneu\t\n",
            "Generated name $chaunica\t\n",
            "Generated name $tammer spider\t\n",
            "Generated name $prow van\t\n",
            "Generated name $rrick ratgo\t\n",
            "Generated name $inpen smars\t\n",
            "609/609 - 49s - loss: 2.0700 - accuracy: 0.3727 - val_loss: 2.1433 - val_accuracy: 0.3639\n",
            "Epoch 2/100\n",
            "Generated name $olvie of girver\t\n",
            "Generated name $thirkrar\t\n",
            "Generated name $stroman\t\n",
            "Generated name $the kuldrake\t\n",
            "Generated name $lavid tom zeno\t\n",
            "Generated name $green char\t\n",
            "Generated name $darkdove\t\n",
            "Generated name $hetubisandel\t\n",
            "Generated name $ssansk\t\n",
            "Generated name $ren san\t\n",
            "609/609 - 47s - loss: 2.0723 - accuracy: 0.3725 - val_loss: 2.1439 - val_accuracy: 0.3641\n",
            "Epoch 3/100\n",
            "Generated name $batman\t\n",
            "Generated name $carl waredaones\t\n",
            "Generated name $share straue\t\n",
            "Generated name $landrakg\t\n",
            "Generated name $rrizot\t\n",
            "Generated name $fames night\t\n",
            "Generated name $slowlam\t\n",
            "Generated name $marto\t\n",
            "Generated name $humk\t\n",
            "Generated name $rircho kenseis\t\n",
            "609/609 - 49s - loss: 2.0675 - accuracy: 0.3726 - val_loss: 2.1478 - val_accuracy: 0.3649\n",
            "Epoch 4/100\n",
            "Generated name $viconic\t\n",
            "Generated name $kokind knight\t\n",
            "Generated name $flambian\t\n",
            "Generated name $scaduscine\t\n",
            "Generated name $boob\t\n",
            "Generated name $brask oncleman\t\n",
            "Generated name $spectroy\t\n",
            "Generated name $ban panin\t\n",
            "Generated name $quidzerit nevil ascarus\t\n",
            "Generated name $cabibac king\t\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-232cafc0f3a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(X_train, y_train, \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           callbacks=[MakeNamesCallback(tokenizer=tokenizer)])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/TLN/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/TLN/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/TLN/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/TLN/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/TLN/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m~/miniconda3/envs/TLN/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/TLN/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "source": [
        "### Generazione dei Nomi\n",
        "\n",
        "Sono state definite due funzioni con differenti strategie per la generazione dei nomi:\n",
        "\n",
        "* 'generate_name_from_seed': dato un seed iniziale, ovvero una sequenza di caratteri, l'i-esimo carattere $c_i$ generato sarà quello che massimizza la probabilità:\n",
        "\n",
        "$$ c_i = \\operatorname*{argmax}_i P(c_i | c_{i-1},\\ldots, c_0) $$\n",
        "\n",
        "* 'generate_name_greedy': dato un seed iniziale, ovvero una sequenza di caratteri, l'i-esimo carattere $c_i$ generato è dato da una v.a.\n",
        "\n",
        "$$ C \\sim P(c_i | c_{i-1},\\ldots, c_0) $$\n",
        "\n",
        "Questa seconda implementazione, è la versione non-deterministica della prima, in quanto ad ogni step viene campionato dal vocabolario un carattere secondo la distribuzione definita dalla rete (l'output della softmax).\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.load_model('data/LM-RNN-model')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ReutPDyOAP"
      },
      "source": [
        "def generate_name_from_seed(tokenizer, model, seed=BEGIN_NAME_TOKEN):\n",
        "  name = seed\n",
        "  for i in range(0, 50):\n",
        "    seq = name_to_seq(name, tokenizer)\n",
        "    #print(seq)\n",
        "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences([seq], padding='pre',\n",
        "                                                              maxlen=max_seq_len-1,value=PAD_CODE)\n",
        "  \n",
        "    prediction = model.predict(padded_seq)[0]\n",
        "    best_idx = tf.argmax(prediction).numpy()\n",
        "    best_char = tokenizer.index_word[best_idx]\n",
        "    #print(best_char)\n",
        "    name += best_char\n",
        "\n",
        "    if best_char == END_NAME_TOKEN:\n",
        "      break\n",
        "  return name\n",
        "\n",
        "def generate_name_greedy(tokenizer, model, seed=BEGIN_NAME_TOKEN):\n",
        "  name = seed\n",
        "  for i in range(0, 50):\n",
        "    seq = name_to_seq(name, tokenizer)\n",
        "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences([seq], padding='pre',\n",
        "                                                              maxlen=max_seq_len-1,value=PAD_CODE)\n",
        "  \n",
        "    prediction = model.predict(padded_seq)[0]\n",
        "    index_vocab = list(tokenizer.index_word.keys())\n",
        "    best_idx = np.random.choice(index_vocab, p=prediction[1:]) # the 0-index is the softmax output for the pad character\n",
        "    best_char = tokenizer.index_word[best_idx]\n",
        "\n",
        "    name += best_char\n",
        "\n",
        "    if best_char == END_NAME_TOKEN:\n",
        "      break\n",
        "  return name"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "source": [],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK45tL6bJB8T",
        "outputId": "4959cbc8-f2f1-41f9-ff9f-3c108d7d4ae2"
      },
      "source": [
        "import itertools\n",
        "\n",
        "# generate names from permutation constructed seeds\n",
        "for i in range(1,3):\n",
        "  for perm in itertools.permutations(\"aemo\",i):\n",
        "    seed = \"\".join(perm)\n",
        "    name = generate_name_from_seed(tokenizer,model, seed=seed)\n",
        "    print(name)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aranno\t\n",
            "ewhor black\t\n",
            "maracan\t\n",
            "owson man\t\n",
            "aeron man\t\n",
            "ampha sand\t\n",
            "aomed\t\n",
            "eara\t\n",
            "emparder sand\t\n",
            "eosed grey\t\n",
            "maracan\t\n",
            "mestar black\t\n",
            "moss black cat\t\n",
            "oard boy\t\n",
            "oed man man\t\n",
            "ommer the sparn spider\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Owqur8OK1oTp",
        "outputId": "b92a3367-4d9b-4e77-b256-0d59a50331e3"
      },
      "source": [
        "for i in range(25):\n",
        "    name = generate_name_greedy(tokenizer, model, seed=BEGIN_NAME_TOKEN)\n",
        "    print(name)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$knini burtie\t\n",
            "$ragus eld woor\t\n",
            "$brtera\t\n",
            "$megomankam javo\t\n",
            "$rosaon\t\n",
            "$coldepsi achora\t\n",
            "$thon xuka\t\n",
            "$sharm\t\n",
            "$cyrsentis\t\n",
            "$krowpler\t\n",
            "$tompon skyven\t\n",
            "$spodon\t\n",
            "$the mordiss\t\n",
            "$rig demon\t\n",
            "$spurk roy\t\n",
            "$spy\t\n",
            "$flif gillawk\t\n",
            "$superman\t\n",
            "$elon cara\t\n",
            "$jesse shaho\t\n",
            "$genseor\t\n",
            "$packmon bwutiter\t\n",
            "$maxs luctor\t\n",
            "$arico\t\n",
            "$yeldshoy\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcb5g4L3We6o"
      },
      "source": [
        "### Risultati\n",
        "\n",
        "* è interessante notare come alcuni dei nomi generati, sopratutto non nelle prime epoche, presentino delle feature tipiche dei nomi di supereroi: il finale \"man\", la particella \"the\", i prefissi \"dr\" e \"mr\", fenomeno che segnale come la rete effettivamente stia apprendendo particolari pattern presenti nel dataset.\n",
        "\n",
        "* in alcuni casi si possono ritrovare nomi generati di veri supereroi \"superman\", \"hulk\", \"ultron\", \"thor\" fenomeno (forse) indicativo di un possibile overfitting\n",
        "\n",
        "* Anche nelle epoche finale, vengono comunque generati nomi \"spuri\"  "
      ]
    }
  ]
}